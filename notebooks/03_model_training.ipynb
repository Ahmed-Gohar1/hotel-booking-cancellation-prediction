{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d0941b",
   "metadata": {},
   "source": [
    "# Hotel Booking Demand - Model Training\n",
    "\n",
    "This notebook trains machine learning models to predict hotel booking cancellations.\n",
    "\n",
    "**Goal**: Build and compare models for cancellation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829d03d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4e5148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d3df8",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3b07f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "==================================================\n",
      "Training set: (95512, 29)\n",
      "Test set: (23878, 29)\n",
      "Number of features: 29\n",
      "\n",
      "Cancellation rate in training: 37.04%\n",
      "Cancellation rate in test: 37.04%\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data\n",
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')['is_canceled']\n",
    "y_test = pd.read_csv('data/y_test.csv')['is_canceled']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nCancellation rate in training: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Cancellation rate in test: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115bdb1",
   "metadata": {},
   "source": [
    "## 3. Handle Any Remaining Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3676ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing values...\n",
      "X_train NaN count: 4\n",
      "X_test NaN count: 0\n",
      "\n",
      "⚠️  Found NaN values - filling with 0\n",
      "✓ NaN values handled\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Checking for missing values...\")\n",
    "print(f\"X_train NaN count: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"X_test NaN count: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any remaining NaN values with 0 (safe for scaled data)\n",
    "if X_train.isnull().sum().sum() > 0 or X_test.isnull().sum().sum() > 0:\n",
    "    print(\"\\n⚠️  Found NaN values - filling with 0\")\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    print(\"✓ NaN values handled\")\n",
    "else:\n",
    "    print(\"✓ No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcebb3",
   "metadata": {},
   "source": [
    "## 4. Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9d68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "✓ Logistic Regression trained\n",
      "✓ Logistic Regression trained\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"✓ Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3d8db",
   "metadata": {},
   "source": [
    "## 5. Evaluate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_metrics = {\n",
    "    'model': 'Logistic Regression',\n",
    "    'train_accuracy': accuracy_score(y_train, lr_train_pred),\n",
    "    'test_accuracy': accuracy_score(y_test, lr_test_pred),\n",
    "    'test_precision': precision_score(y_test, lr_test_pred),\n",
    "    'test_recall': recall_score(y_test, lr_test_pred),\n",
    "    'test_f1': f1_score(y_test, lr_test_pred)\n",
    "}\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {lr_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {lr_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {lr_metrics['test_precision']:.4f}\")\n",
    "print(f\"Test Recall: {lr_metrics['test_recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {lr_metrics['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f523b",
   "metadata": {},
   "source": [
    "## 6. Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"✓ Random Forest trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b33a38",
   "metadata": {},
   "source": [
    "## 7. Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05beb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rf_metrics = {\n",
    "    'model': 'Random Forest',\n",
    "    'train_accuracy': accuracy_score(y_train, rf_train_pred),\n",
    "    'test_accuracy': accuracy_score(y_test, rf_test_pred),\n",
    "    'test_precision': precision_score(y_test, rf_test_pred),\n",
    "    'test_recall': recall_score(y_test, rf_test_pred),\n",
    "    'test_f1': f1_score(y_test, rf_test_pred)\n",
    "}\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {rf_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {rf_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {rf_metrics['test_precision']:.4f}\")\n",
    "print(f\"Test Recall: {rf_metrics['test_recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {rf_metrics['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80715f9f",
   "metadata": {},
   "source": [
    "## 8. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d369bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([lr_metrics, rf_metrics])\n",
    "comparison_df = comparison_df.set_index('model')\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Determine best model based on F1 score\n",
    "best_model_name = comparison_df['test_f1'].idxmax()\n",
    "print(f\"\\n✓ Best model: {best_model_name}\")\n",
    "print(f\"  (based on Test F1-Score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd7e49",
   "metadata": {},
   "source": [
    "## 9. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be745022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_names = joblib.load('artifacts/feature_names.joblib')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(\"=\" * 50)\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9303a5",
   "metadata": {},
   "source": [
    "## 10. Save Models and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e81ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(lr_model, 'artifacts/lr_model.joblib')\n",
    "joblib.dump(rf_model, 'artifacts/rf_model.joblib')\n",
    "\n",
    "# Save best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    joblib.dump(rf_model, 'artifacts/best_model.joblib')\n",
    "else:\n",
    "    joblib.dump(lr_model, 'artifacts/best_model.joblib')\n",
    "\n",
    "# Save metrics\n",
    "comparison_df.to_csv('artifacts/model_metrics.csv')\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('artifacts/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"✓ Models and artifacts saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - artifacts/lr_model.joblib\")\n",
    "print(\"  - artifacts/rf_model.joblib\")\n",
    "print(\"  - artifacts/best_model.joblib\")\n",
    "print(\"  - artifacts/model_metrics.csv\")\n",
    "print(\"  - artifacts/feature_importance.csv\")\n",
    "print(f\"\\n✓ Project complete! Best model: {best_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
